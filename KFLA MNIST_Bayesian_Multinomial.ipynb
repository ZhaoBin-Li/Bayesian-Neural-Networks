{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(\"../\") / \"BayesLogReg\" / \"bayes_logistic\" / \"pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir / 'x_extract.pickle', 'rb') as f_x, open(save_dir / 'y_extract.pickle', 'rb') as f_y:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    x_extract = pickle.load(f_x)\n",
    "    y_extract = pickle.load(f_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 0. 4. 1. 9. 2. 1. 3. 1. 4. 3. 5. 3. 6. 1. 7. 2. 8. 6. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(y_extract['train'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7. 2. 1. 0. 4. 1. 4. 9. 5. 9. 0. 6. 9. 0. 1. 5. 9. 7. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(y_extract['val'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([5., 0., 4., ..., 5., 6., 8.]),\n",
       " 'val': array([7., 2., 1., ..., 4., 5., 6.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([[22.48517418, 14.61251354, 27.79730988, ..., 22.38022995,\n",
       "         24.60155106, 20.74303436],\n",
       "        [30.97985077, 17.52405548, 24.48457146, ..., 20.76237869,\n",
       "         25.33365631, 19.87883377],\n",
       "        [19.80603027, 20.39133835, 17.72837067, ..., 16.84418106,\n",
       "         19.95091057, 19.42064476],\n",
       "        ...,\n",
       "        [17.22813606, 12.17297745, 23.73967361, ..., 19.77940559,\n",
       "         19.77432823, 19.95170784],\n",
       "        [17.67393494, 14.05826092, 17.64319038, ..., 13.33616924,\n",
       "         16.925951  , 14.98935986],\n",
       "        [17.06033897, 13.15117264, 16.17965317, ..., 17.99224663,\n",
       "         19.5974369 , 18.54821396]]),\n",
       " 'val': array([[12.32572365, 15.35899067, 12.46453094, ..., 23.22957993,\n",
       "         10.54736519, 18.8499279 ],\n",
       "        [22.80735397, 20.179739  , 26.58662224, ..., 14.57098484,\n",
       "         22.49274445, 19.51333618],\n",
       "        [ 9.86898041, 17.40467262,  8.67834949, ..., 12.12457466,\n",
       "         10.97650528, 11.81525993],\n",
       "        ...,\n",
       "        [25.49707222, 16.16849518, 19.02800179, ..., 24.16654015,\n",
       "         27.76334763, 30.31781006],\n",
       "        [16.7231884 , 11.41403866, 20.34407043, ..., 19.74565697,\n",
       "         21.24572182, 16.93716812],\n",
       "        [34.31462097, 26.21321678, 33.69176483, ..., 23.06213379,\n",
       "         31.47667122, 28.26307106]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_extract['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_extract['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_extract['train'] = scaler.fit_transform(x_extract['train'])\n",
    "x_extract['val'] = scaler.transform(x_extract['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from torch.jit import script, trace       # hybrid frontend decorator and tracing jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_2L_KFRA(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc3 = nn.Linear(self.input_dim + 1, self.output_dim, bias=False)\n",
    "\n",
    "        # choose your non linearity\n",
    "        self.a2 = None\n",
    "        self.h3 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        one = x.new(x.shape[0], 1).fill_(1)\n",
    "        self.a2 = x\n",
    "        self.a2 = torch.cat((self.a2.data, one), dim=1)\n",
    "        self.h3 = self.fc3(self.a2)\n",
    "\n",
    "        return self.h3\n",
    "\n",
    "    def sample_predict(self, x, Nsamples, Qinv3, HHinv3, MAP3):\n",
    "        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n",
    "        one = x.new(x.shape[0], 1).fill_(1)\n",
    "        self.a2 = x\n",
    "        self.a2 = torch.cat((self.a2.data, one), dim=1)\n",
    "        w3_list = MAP3.data.new(Nsamples, MAP3.size(0), MAP3.size(1))\n",
    "        for i in range(Nsamples):\n",
    "            w3 = sample_K_laplace_MN(MAP3, Qinv3, HHinv3)\n",
    "            y = torch.matmul(self.a2, torch.t(w3))\n",
    "            predictions[i] = y\n",
    "            w3_list[i] = w3\n",
    "\n",
    "        return predictions, w3_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_CE_preact_hessian(last_layer_acts):\n",
    "    side = last_layer_acts.shape[1]\n",
    "    I = torch.eye(side).type(torch.ByteTensor)\n",
    "    # for i != j    H = -ai * aj -- Note that these are activations not pre-activations\n",
    "    Hl = - last_layer_acts.unsqueeze(1) * last_layer_acts.unsqueeze(2)\n",
    "    # for i == j    H = ai * (1 - ai)\n",
    "    Hl[:, I] = last_layer_acts*(1-last_layer_acts)\n",
    "    return Hl\n",
    "\n",
    "def layer_act_hessian_recurse(prev_hessian, prev_weights, layer_pre_acts):\n",
    "    \n",
    "    newside = layer_pre_acts.shape[1]\n",
    "    batch_size = layer_pre_acts.shape[0]\n",
    "    I = torch.eye(newside).type(torch.ByteTensor) # .unsqueeze(0).expand([batch_size, -1, -1])\n",
    "    \n",
    "#     print(d_act(layer_pre_acts).unsqueeze(1).shape, I.shape)\n",
    "    B = prev_weights.data.new(batch_size, newside, newside).fill_(0)\n",
    "    B[:, I] = (layer_pre_acts > 0).type(B.type()) # d_act(layer_pre_acts)\n",
    "    D = prev_weights.data.new(batch_size, newside, newside).fill_(0) # is just 0 for a piecewise linear\n",
    "#     D[:, I] = dd_act(layer_pre_acts) * act_grads\n",
    "\n",
    "    Hl = torch.bmm(torch.t(prev_weights).unsqueeze(0).expand([batch_size, -1, -1]), prev_hessian)    \n",
    "    Hl = torch.bmm(Hl, prev_weights.unsqueeze(0).expand([batch_size, -1, -1]))\n",
    "    Hl = torch.bmm(B, Hl)\n",
    "    Hl = torch.matmul(Hl, B)\n",
    "    Hl = Hl + D\n",
    "    \n",
    "    return Hl\n",
    "\n",
    "def chol_scale_invert_kron_factor(factor, prior_scale, data_scale, upper=False):\n",
    "    \n",
    "    scaled_factor = data_scale * factor + prior_scale * torch.eye(factor.shape[0]).type(factor.type())\n",
    "    inv_factor = torch.inverse(scaled_factor)\n",
    "    chol_inv_factor = torch.cholesky(inv_factor, upper=upper)\n",
    "    return chol_inv_factor\n",
    "\n",
    "def sample_K_laplace_MN(MAP, upper_Qinv, lower_HHinv):\n",
    "    # H = Qi (kron) HHi\n",
    "    # sample isotropic unit variance mtrix normal\n",
    "    Z = MAP.data.new(MAP.size()).normal_(mean=0, std=1)\n",
    "    # AAT = HHi\n",
    "#     A = torch.cholesky(HHinv, upper=False)\n",
    "    # BTB = Qi\n",
    "#     B = torch.cholesky(Qinv, upper=True)\n",
    "    all_mtx_sample = MAP + torch.matmul(torch.matmul(lower_HHinv, Z), upper_Qinv)\n",
    "    \n",
    "    return all_mtx_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nNet:')\n",
    "\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBayes_Net(BaseNet):\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, side_in=10, cuda=True, classes=10, n_hid=1200, batch_size=128, prior_sig=0):\n",
    "        super(KBayes_Net, self).__init__()\n",
    "        cprint('y', ' Creating Net!! ')\n",
    "        self.lr = lr\n",
    "        self.cuda = cuda\n",
    "        self.side_in=side_in\n",
    "        self.prior_sig = prior_sig\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "\n",
    "        self.model = Linear_2L_KFRA(input_dim=self.side_in, output_dim=self.classes)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "#             cudnn.benchmark = True\n",
    "\n",
    "        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n",
    "\n",
    "    def create_opt(self):\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.5, weight_decay=1/self.prior_sig**2)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = x.cuda(), y.long().cuda()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "            \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err\n",
    "\n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = x.cuda(), y.long().cuda()\n",
    "\n",
    "        out = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "    \n",
    "    \n",
    "    def get_K_laplace_params(self, trainloader):\n",
    "        self.model.eval()\n",
    "\n",
    "        it_counter = 0\n",
    "        cum_HH3 = self.model.fc3.weight.data.new(self.model.output_dim, self.model.output_dim).fill_(0)\n",
    "        cum_Q3 = self.model.fc3.weight.data.new(self.model.input_dim+1, self.model.input_dim+1).fill_(0)\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            x, y = x.cuda(), y.long().cuda()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self.model(x)\n",
    "            out_act = F.softmax(out, dim=1)\n",
    "\n",
    "            #     ------------------------------------------------------------------\n",
    "            HH3 = softmax_CE_preact_hessian(out_act.data)\n",
    "            cum_HH3 += HH3.sum(dim=0)\n",
    "\n",
    "            Q3 = torch.bmm(self.model.a2.data.unsqueeze(2), self.model.a2.data.unsqueeze(1))\n",
    "            cum_Q3 += Q3.sum(dim=0)\n",
    "            #     ------------------------------------------------------------------\n",
    "            it_counter += x.shape[0]\n",
    "            print(it_counter)\n",
    "\n",
    "        EHH3 = cum_HH3 / it_counter\n",
    "\n",
    "        EQ3 = cum_Q3 / it_counter\n",
    "\n",
    "        MAP3 = self.model.fc3.weight.data\n",
    "        \n",
    "        return EQ3, EHH3, MAP3\n",
    "    \n",
    "    def sample_eval(self, x, y, Nsamples, scale_inv_EQ3, scale_inv_EHH3, MAP3):\n",
    "        x, y = x.cuda(), y.long().cuda()\n",
    "\n",
    "        out, w3_list = self.model.sample_predict(x, Nsamples, scale_inv_EQ3, scale_inv_EHH3, MAP3)\n",
    "        \n",
    "        mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "        probs = mean_out.data.cpu()\n",
    "\n",
    "        log_mean_probs_out = torch.log(mean_out)\n",
    "        loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs, w3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "  \n",
    "def cprint(color, text, **kwargs):\n",
    "    if color[0] == '*':\n",
    "        pre_code = '1;'\n",
    "        color = color[1:]\n",
    "    else:\n",
    "        pre_code = ''\n",
    "    code = {\n",
    "        'a': '30',\n",
    "        'r': '31',\n",
    "        'g': '32',\n",
    "        'y': '33',\n",
    "        'b': '34',\n",
    "        'p': '35',\n",
    "        'c': '36',\n",
    "        'w': '37'\n",
    "    }\n",
    "    print(\"\\x1b[%s%sm%s\\x1b[0m\" % (pre_code, code[color], text), **kwargs)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 0.00M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl430/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0/20, Jtr = 4.029546, err = 0.605417, \u001b[31m   time: 0.814029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 1.687383, err = 0.293500\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl430/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Linear_2L_KFRA. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1/20, Jtr = 1.103602, err = 0.241850, \u001b[31m   time: 0.432569 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.615995, err = 0.175400\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/20, Jtr = 0.544743, err = 0.152617, \u001b[31m   time: 0.424394 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.459049, err = 0.130500\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/20, Jtr = 0.455274, err = 0.128350, \u001b[31m   time: 0.381745 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.418795, err = 0.118800\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/20, Jtr = 0.425400, err = 0.120017, \u001b[31m   time: 0.415982 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.400490, err = 0.116400\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/20, Jtr = 0.411549, err = 0.117250, \u001b[31m   time: 0.459068 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.392299, err = 0.114600\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 6/20, Jtr = 0.402945, err = 0.115467, \u001b[31m   time: 0.402985 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.387726, err = 0.114300\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 7/20, Jtr = 0.398804, err = 0.115083, \u001b[31m   time: 0.470389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.382825, err = 0.113100\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 8/20, Jtr = 0.395835, err = 0.114517, \u001b[31m   time: 0.445076 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.379388, err = 0.112600\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 9/20, Jtr = 0.391791, err = 0.114067, \u001b[31m   time: 0.407381 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.377465, err = 0.113300\n",
      "\u001b[0m\n",
      "it 10/20, Jtr = 0.388282, err = 0.113350, \u001b[31m   time: 0.397794 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.378062, err = 0.113400\n",
      "\u001b[0m\n",
      "it 11/20, Jtr = 0.386399, err = 0.112533, \u001b[31m   time: 0.432761 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.372498, err = 0.110600\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 12/20, Jtr = 0.388676, err = 0.114217, \u001b[31m   time: 0.461651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.375289, err = 0.111500\n",
      "\u001b[0m\n",
      "it 13/20, Jtr = 0.385034, err = 0.112650, \u001b[31m   time: 0.472793 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.370869, err = 0.110600\n",
      "\u001b[0m\n",
      "it 14/20, Jtr = 0.384344, err = 0.112617, \u001b[31m   time: 0.400316 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.372997, err = 0.109800\n",
      "\u001b[0m\n",
      "\u001b[36mWritting models_KLaplace_NN_MNIST/theta_best.dat\n",
      "\u001b[0m\n",
      "it 15/20, Jtr = 0.383398, err = 0.111917, \u001b[31m   time: 0.421798 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.373599, err = 0.111800\n",
      "\u001b[0m\n",
      "it 16/20, Jtr = 0.383153, err = 0.112817, \u001b[31m   time: 0.414752 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.370122, err = 0.111000\n",
      "\u001b[0m\n",
      "it 17/20, Jtr = 0.385539, err = 0.114050, \u001b[31m   time: 0.459669 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.369703, err = 0.110600\n",
      "\u001b[0m\n",
      "it 18/20, Jtr = 0.381798, err = 0.112433, \u001b[31m   time: 0.434484 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.369868, err = 0.109800\n",
      "\u001b[0m\n",
      "it 19/20, Jtr = 0.381484, err = 0.111650, \u001b[31m   time: 0.506854 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.368518, err = 0.110400\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 0.763480 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib\n",
    "\n",
    "models_dir = 'models_KLaplace_NN_MNIST'\n",
    "results_dir = 'results_KLaplace_NN_MNIST'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "batch_size = 10000\n",
    "nb_epochs = 20 # We can do less iterations as this method has faster convergence\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(torch.tensor(x_extract['train'], dtype=torch.float), \n",
    "                                          torch.tensor(y_extract['train'], dtype=torch.long))\n",
    "\n",
    "valset = torch.utils.data.TensorDataset(torch.tensor(x_extract['val'], dtype=torch.float), \n",
    "                                         torch.tensor(y_extract['val'], dtype=torch.long))\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "prior_sig = 1\n",
    "########################################################################################\n",
    "net = KBayes_Net(lr=lr, side_in=10, cuda=use_cuda, classes=10, batch_size=batch_size, prior_sig=prior_sig)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    \n",
    "    net.set_mode_train(True)\n",
    "\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        cost_train[i] += cost\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr = %f, err = %f, \" % (i, nb_epochs, cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "            cost, err, _ = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "EQ3, EHH3, MAP3 = net.get_K_laplace_params(trainloader)\n",
    "h_params = [EQ3, EHH3, MAP3]\n",
    "\n",
    "with open(models_dir + '/block_hessian_params.pkl', 'wb') as f:\n",
    "        pickle.dump(h_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0323, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EHH3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0200, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EQ3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scale = np.sqrt(60000)\n",
    "\n",
    "prior_prec = 1/prior_sig**2\n",
    "prior_scale = np.sqrt(prior_prec)\n",
    "\n",
    "scale_inv_EQ3 = chol_scale_invert_kron_factor(EQ3, prior_scale, data_scale, upper=True)\n",
    "scale_inv_EHH3 = chol_scale_invert_kron_factor(EHH3, prior_scale, data_scale, upper=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m    Loglike = -3685.184082, err = 0.110400\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nb_samples = 0    \n",
    "\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "test_predictions = np.zeros((10000, 10))\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.eval(x, y) # \n",
    "\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_test_predictions = test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m    Loglike = -3685.733398, err = 0.11060000\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nb_samples = 0    \n",
    "\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "test_predictions = np.zeros((10000, 10))\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "Nsamples = 10000\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs, w3_list = net.sample_eval(x, y, Nsamples, scale_inv_EQ3, scale_inv_EHH3, MAP3)\n",
    "\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "    break\n",
    "\n",
    "test_err /= nb_samples\n",
    "cprint('b', '    Loglike = %5.6f, err = %1.8f\\n' % (-test_cost, test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Laplace_test_predictions = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_probs_idx = np.argmax(Laplace_test_predictions, axis=1)\n",
    "M_probs_idx = np.argmax(MAP_test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_probs = np.max(Laplace_test_predictions, axis=1)\n",
    "M_probs = np.max(MAP_test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000259663088619709"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(L_probs - M_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
